import torchaudio
import tempfile
from typing import Optional, List
import torch
import os
import ast
import sys
import librosa

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

from pipeline_ace_step import ACEStepPipeline as AP

import folder_paths
cache_dir = folder_paths.get_temp_directory()
models_dir = folder_paths.models_dir
model_path = os.path.join(models_dir, "TTS", "ACE-Step-v1-3.5B")


class AudioCacher:
    """
    ä¸€ä¸ªç”¨äºŽç¼“å­˜éŸ³é¢‘å¼ é‡åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œå¹¶åœ¨ä¹‹åŽæ¸…ç†è¿™äº›æ–‡ä»¶çš„ç±»ã€‚
    æ”¯æŒä½œä¸ºä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨ï¼Œä»¥ä¾¿è‡ªåŠ¨æ¸…ç†ã€‚
    """
    def __init__(self, cache_dir: Optional[str] = None, default_format: str = "wav"):
        """
        åˆå§‹åŒ– AudioCacherã€‚

        Args:
            cache_dir (Optional[str]): ç¼“å­˜æ–‡ä»¶å­˜æ”¾çš„ç›®å½•ã€‚
                                       å¦‚æžœä¸º Noneï¼Œåˆ™ä½¿ç”¨ç³»ç»Ÿé»˜è®¤çš„ä¸´æ—¶ç›®å½•ã€‚
            default_format (str): é»˜è®¤çš„éŸ³é¢‘æ–‡ä»¶æ ¼å¼åŽç¼€ (ä¾‹å¦‚ "wav", "mp3", "flac")ã€‚
        """
        if cache_dir is None:
            self.cache_dir = tempfile.gettempdir()
        else:
            self.cache_dir = cache_dir
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        if not os.path.exists(self.cache_dir):
            try:
                os.makedirs(self.cache_dir, exist_ok=True)
            except OSError as e:
                raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå…³é”®çš„åˆå§‹åŒ–æ­¥éª¤
        self.default_format = default_format.lstrip('.') # ç¡®ä¿æ²¡æœ‰å‰å¯¼ç‚¹
        self._files_to_cleanup_in_context: List[str] = [] # ç”¨äºŽä¸Šä¸‹æ–‡ç®¡ç†å™¨

    def cache_audio_tensor(
        self,
        audio_tensor: torch.Tensor,
        sample_rate: int,
        filename_prefix: str = "cached_audio_",
        audio_format: Optional[str] = None
    ) -> str:
        """
        å°†éŸ³é¢‘å¼ é‡ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶ï¼Œå¹¶è¿”å›žæ–‡ä»¶è·¯å¾„ã€‚

        Args:
            audio_tensor (torch.Tensor): è¦ä¿å­˜çš„éŸ³é¢‘å¼ é‡ã€‚
            sample_rate (int): éŸ³é¢‘çš„é‡‡æ ·çŽ‡ã€‚
            filename_prefix (str): ç¼“å­˜æ–‡ä»¶åçš„å‰ç¼€ã€‚
            audio_format (Optional[str]): è¦ä½¿ç”¨çš„éŸ³é¢‘æ ¼å¼ (ä¾‹å¦‚ "wav", "mp3")ã€‚
                                       å¦‚æžœä¸º Noneï¼Œåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶è®¾ç½®çš„ default_formatã€‚

        Returns:
            str: ä¿å­˜çš„ç¼“å­˜æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚

        Raises:
            RuntimeError: å¦‚æžœä¿å­˜éŸ³é¢‘å¤±è´¥ã€‚
        """
        current_format = (audio_format or self.default_format).lstrip('.')
        # åˆ›å»ºä¸€ä¸ªå¸¦ç‰¹å®šåŽç¼€çš„ä¸´æ—¶æ–‡ä»¶ï¼Œä½†ä¸ç«‹å³åˆ é™¤
        # NamedTemporaryFile ä¼šåœ¨åˆ›å»ºæ—¶æ‰“å¼€æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå…³é—­å®ƒæ‰èƒ½è®© torchaudio.save ä½¿ç”¨
        try:
            with tempfile.NamedTemporaryFile(
                prefix=filename_prefix,
                suffix=f".{current_format}",
                dir=self.cache_dir,
                delete=False  # è¿™æ˜¯å…³é”®ï¼Œæˆ‘ä»¬æ‰‹åŠ¨ç®¡ç†åˆ é™¤
            ) as tmp_file:
                temp_filepath = tmp_file.name
            # æ­¤æ—¶ tmp_file å·²ç»å…³é—­ï¼Œä½†æ–‡ä»¶å›  delete=False è€Œä¿ç•™
            torchaudio.save(temp_filepath, audio_tensor, sample_rate)
            # å¦‚æžœåœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ä¸­ä½¿ç”¨ï¼Œåˆ™è®°å½•æ­¤æ–‡ä»¶ä»¥å¤‡è‡ªåŠ¨æ¸…ç†
            self._files_to_cleanup_in_context.append(temp_filepath)
            return temp_filepath
        except Exception as e:
            # å¦‚æžœ temp_filepath å·²å®šä¹‰ä¸”æ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•åˆ é™¤ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¸å®Œæ•´æˆ–æŸå
            if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                try:
                    os.remove(temp_filepath)
                except OSError as e_clean:
                    logger.warning(f"Error cleaning temporary file {temp_filepath}: {e_clean}")
            raise RuntimeError(f"Failed to save audio: {e}") from e

    def cleanup_file(self, filepath: str) -> bool:
        """
        æ¸…ç†æŒ‡å®šçš„ç¼“å­˜æ–‡ä»¶ã€‚

        Args:
            filepath (str): è¦åˆ é™¤çš„æ–‡ä»¶çš„è·¯å¾„ã€‚

        Returns:
            bool: å¦‚æžœæ–‡ä»¶æˆåŠŸåˆ é™¤æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™è¿”å›ž Trueï¼›å¦‚æžœåˆ é™¤å¤±è´¥ï¼Œåˆ™è¿”å›ž Falseã€‚
        """
        if not filepath:
            return True # æ²¡æœ‰æ–‡ä»¶å¯ä»¥åˆ é™¤ï¼Œæ‰€ä»¥è®¤ä¸ºæ˜¯â€œæˆåŠŸâ€
        if os.path.exists(filepath):
            try:
                os.remove(filepath)
                # å¦‚æžœæ–‡ä»¶åœ¨ä¸Šä¸‹æ–‡ä¸­è¢«è·Ÿè¸ªï¼Œä¹Ÿä»Žä¸­ç§»é™¤
                if filepath in self._files_to_cleanup_in_context:
                    self._files_to_cleanup_in_context.remove(filepath)
                return True
            except OSError as e:
                return False
        else:
            # å¦‚æžœæ–‡ä»¶åœ¨ä¸Šä¸‹æ–‡ä¸­è¢«è·Ÿè¸ªï¼Œä¹Ÿä»Žä¸­ç§»é™¤
            if filepath in self._files_to_cleanup_in_context:
                self._files_to_cleanup_in_context.remove(filepath)
            return True # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¹Ÿè§†ä¸ºæ¸…ç†â€œæˆåŠŸâ€

    def cleanup_all_tracked_files(self) -> None:
        """
        æ¸…ç†æ‰€æœ‰ç”±å½“å‰ä¸Šä¸‹æ–‡ç®¡ç†å™¨å®žä¾‹è·Ÿè¸ªçš„ç¼“å­˜æ–‡ä»¶ã€‚
        """
        # è¿­ä»£åˆ—è¡¨çš„å‰¯æœ¬ï¼Œå› ä¸º cleanup_file å¯èƒ½ä¼šä¿®æ”¹åˆ—è¡¨
        for f_path in list(self._files_to_cleanup_in_context):
            self.cleanup_file(f_path)
        self._files_to_cleanup_in_context.clear() # ç¡®ä¿åˆ—è¡¨è¢«æ¸…ç©º

    def __enter__(self):
        """è¿›å…¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ—¶è°ƒç”¨ã€‚"""
        # é‡ç½®è·Ÿè¸ªæ–‡ä»¶åˆ—è¡¨ï¼Œä»¥é˜²åŒä¸€ä¸ªå®žä¾‹è¢«å¤šæ¬¡ç”¨äºŽ 'with' è¯­å¥
        self._files_to_cleanup_in_context = []
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡ç®¡ç†å™¨æ—¶è°ƒç”¨ï¼Œè´Ÿè´£æ¸…ç†ã€‚"""
        self.cleanup_all_tracked_files()
        # è¿”å›ž False ä»¥ä¾¿åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶é‡æ–°æŠ›å‡ºå¼‚å¸¸
        return False


from data_sampler import DataSampler

def sample_data(json_data):
    return (
            json_data["audio_duration"],
            json_data["prompt"],
            json_data["lyrics"],
            json_data["infer_step"],
            json_data["guidance_scale"],
            json_data["scheduler_type"],
            json_data["cfg_type"],
            json_data["omega_scale"],
            json_data["actual_seeds"][0],
            json_data["guidance_interval"],
            json_data["guidance_interval_decay"],
            json_data["min_guidance_scale"],
            json_data["use_erg_tag"],
            json_data["use_erg_lyric"],
            json_data["use_erg_diffusion"],
            ", ".join(map(str, json_data["oss_steps"])),
            json_data["guidance_scale_text"] if "guidance_scale_text" in json_data else 0.0,
            json_data["guidance_scale_lyric"] if "guidance_scale_lyric" in json_data else 0.0,
            )

data_sampler = DataSampler()

json_data = data_sampler.sample()
json_data = sample_data(json_data)

audio_duration,\
prompt, \
lyrics,\
infer_step, \
guidance_scale,\
scheduler_type, \
cfg_type, \
omega_scale, \
manual_seeds, \
guidance_interval, \
guidance_interval_decay, \
min_guidance_scale, \
use_erg_tag, \
use_erg_lyric, \
use_erg_diffusion, \
oss_steps, \
guidance_scale_text, \
guidance_scale_lyric = json_data


class GenerationParameters:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": 
                    { "audio_duration": ("FLOAT", {"default": audio_duration, "min": 0.0, "max": 240.0, "step": 1.0, "tooltip": "0 is a random length"}),
                      "infer_step": ("INT", {"default": infer_step, "min": 1, "max": 60, "step": 1}),
                      "guidance_scale": ("FLOAT", {"default": guidance_scale, "min": 0.0, "max": 200.0, "step": 0.1, "tooltip": "When guidance_scale_lyric > 1 and guidance_scale_text > 1, the guidance scale will not be applied."}),
                      "scheduler_type": (["euler", "heun"], {"default": scheduler_type, "tooltip": "euler is recommended. heun will take more time."}),
                      "cfg_type": (["cfg", "apg", "cfg_star"], {"default": cfg_type, "tooltip": "apg is recommended. cfg and cfg_star are almost the same."}),
                      "omega_scale": ("FLOAT", {"default": omega_scale, "min": -100.0, "max": 100.0, "step": 0.1, "tooltip": "Higher values can reduce artifacts"}),
                      "seed": ("INT", {"default":manual_seeds, "min": 0, "max": 4294967295, "step": 1}),
                      "guidance_interval": ("FLOAT", {"default": guidance_interval, "min": 0, "max": 1, "step": 0.01, "tooltip": "0.5 means only apply guidance in the middle steps"}),
                      "guidance_interval_decay": ("FLOAT", {"default": guidance_interval_decay, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "Guidance scale will decay from guidance_scale to min_guidance_scale in the interval. 0.0 means no decay."}),
                      "min_guidance_scale": ("INT", {"default": min_guidance_scale, "min": 0, "max": 200, "step": 1}),
                      "use_erg_tag": ("BOOLEAN", {"default": use_erg_tag}),
                      "use_erg_lyric": ("BOOLEAN", {"default": use_erg_lyric}),
                      "use_erg_diffusion": ("BOOLEAN", {"default": use_erg_diffusion}),
                      "oss_steps": ("STRING", {"default": oss_steps}),
                      "guidance_scale_text": ("FLOAT", {"default": guidance_scale_text, "min": 0.0, "max": 10.0, "step": 0.1}),
                      "guidance_scale_lyric": ("FLOAT", {"default": guidance_scale_lyric, "min": 0.0, "max": 10.0, "step": 0.1}),
                    },
                }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("parameters",)
    FUNCTION = "generate"
    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"

    def generate(self, **kwargs):
        kwargs["manual_seeds"] = kwargs.pop("seed")
        return (str(kwargs),)


class MultiLinePromptACES:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "multi_line_prompt": ("STRING", {
                    "multiline": True, 
                    "default": prompt}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("prompt",)
    FUNCTION = "promptgen"
    
    def promptgen(self, multi_line_prompt: str):
        return (multi_line_prompt.strip(),)


class MultiLineLyrics:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "multi_line_prompt": ("STRING", {
                    "multiline": True, 
                    "default": lyrics}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("lyrics",)
    FUNCTION = "lyricsgen"
    
    def lyricsgen(self, multi_line_prompt: str):
        return (multi_line_prompt.strip(),)


class ACEStepGen:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "prompt": ("STRING", {"forceInput": True}),
                "lyrics": ("STRING", {"forceInput": True}),
                "parameters": ("STRING", {"forceInput": True}),
                # "unload_model": ("BOOLEAN", {"default": False}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("music",)
    FUNCTION = "acestepgen"
    
    def acestepgen(self, prompt: str, lyrics: str, parameters: str, unload_model=True):
        
        parameters = ast.literal_eval(parameters)
        ap = AP(model_path)
        audio_output = ap(prompt=prompt, lyrics=lyrics, task="text2music", **parameters)
        audio, sr = audio_output[0][0].unsqueeze(0), audio_output[0][1]
        if unload_model:
            ap.cleanup()
        
        return ({"waveform": audio, "sample_rate": sr},)


class ACEStepRepainting:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "src_audio": ("AUDIO",),
                "prompt": ("STRING", {"forceInput": True}),
                "lyrics": ("STRING", {"forceInput": True}),
                "parameters": ("STRING", {"forceInput": True}),
                "repaint_start": ("INT", {"default": 0, "min": 0, "max": 1000, "step": 1}),
                "repaint_end": ("INT", {"default": 0, "min": 0, "max": 1000, "step": 1}),
                "repaint_variance": ("FLOAT", {"default": 0.01, "min": 0.01, "max": 1.0, "step": 0.01}),
                "seed": ("INT", {"default":0, "min": 0, "max": 4294967295, "step": 1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("music",)
    FUNCTION = "acesteprepainting"
    
    def acesteprepainting(self, src_audio, prompt: str, lyrics: str, parameters: str, repaint_start, repaint_end, repaint_variance, seed, unload_model=True):
        retake_seeds = [str(seed)]
        ac = AudioCacher(cache_dir=cache_dir)
        src_audio_path = ac.cache_audio_tensor(src_audio["waveform"].squeeze(0), src_audio["sample_rate"], filename_prefix="src_audio_")
        
        audio_duration = librosa.get_duration(filename=src_audio_path)
        if repaint_end > audio_duration:
            repaint_end = audio_duration

        parameters = ast.literal_eval(parameters)
        parameters["audio_duration"] = audio_duration

        ap = AP(model_path)
        audio_output = ap(
            prompt=prompt, 
            lyrics=lyrics, 
            task="repaint", 
            retake_seeds=retake_seeds, 
            src_audio_path=src_audio_path, 
            repaint_start=repaint_start, 
            repaint_end=repaint_end, 
            retake_variance=repaint_variance, 
            **parameters)
            
        audio, sr = audio_output[0][0].unsqueeze(0), audio_output[0][1]

        ac.cleanup_file(src_audio_path)
        if unload_model:
            ap.cleanup()
        
        return ({"waveform": audio, "sample_rate": sr},)


class ACEStepEdit:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "src_audio": ("AUDIO",),
                "prompt": ("STRING", {"forceInput": True}),
                "lyrics": ("STRING", {"forceInput": True}),
                "parameters": ("STRING", {"forceInput": True}),
                "edit_prompt": ("STRING", {"forceInput": True}),
                "edit_lyrics": ("STRING", {"forceInput": True}),
                "edit_n_min": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.01}),
                "edit_n_max": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "seed": ("INT", {"default":0, "min": 0, "max": 4294967295, "step": 1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("music",)
    FUNCTION = "acestepedit"
    
    def acestepedit(self, src_audio, prompt: str, lyrics: str, parameters: str, edit_prompt, edit_lyrics, edit_n_min, edit_n_max, seed, unload_model=True):
        retake_seeds = [str(seed)]
        ac = AudioCacher(cache_dir=cache_dir)
        src_audio_path = ac.cache_audio_tensor(src_audio["waveform"].squeeze(0), src_audio["sample_rate"], filename_prefix="src_audio_")
        
        audio_duration = librosa.get_duration(filename=src_audio_path)
        parameters = ast.literal_eval(parameters)
        parameters["audio_duration"] = audio_duration

        ap = AP(model_path)
        audio_output = ap(
            prompt=prompt, 
            lyrics=lyrics, 
            task="edit", 
            retake_seeds=retake_seeds, 
            src_audio_path=src_audio_path, 
            edit_target_prompt = edit_prompt,
            edit_target_lyrics = edit_lyrics,
            edit_n_min = edit_n_min,
            edit_n_max = edit_n_max,
            **parameters)
            
        audio, sr = audio_output[0][0].unsqueeze(0), audio_output[0][1]

        ac.cleanup_file(src_audio_path)
        if unload_model:
            ap.cleanup()
        
        return ({"waveform": audio, "sample_rate": sr},)


class ACEStepExtend:
    @classmethod
    def INPUT_TYPES(cls):
               
        return {
            "required": {
                "src_audio": ("AUDIO",),
                "prompt": ("STRING", {"forceInput": True}),
                "lyrics": ("STRING", {"forceInput": True}),
                "parameters": ("STRING", {"forceInput": True}),
                "left_extend_length": ("INT", {"default": 0, "min": 0, "max": 1000, "step": 1}),
                "right_extend_length": ("INT", {"default": 0, "min": 0, "max": 1000, "step": 1}),
                # "repaint_variance": ("FLOAT", {"default": 0.01, "min": 0.01, "max": 1.0, "step": 0.01}),
                "seed": ("INT", {"default":0, "min": 0, "max": 4294967295, "step": 1}),
                # "unload_model": ("BOOLEAN", {"default": False}),
                },
        }

    CATEGORY = "ðŸŽ¤MW/MW-ACE-Step"
    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("music",)
    FUNCTION = "acestepextend"
    
    def acestepextend(self, src_audio, prompt: str, lyrics: str, parameters: str, left_extend_length, right_extend_length, seed, unload_model=True):
        retake_seeds = [str(seed)]
        ac = AudioCacher(cache_dir=cache_dir)
        src_audio_path = ac.cache_audio_tensor(src_audio["waveform"].squeeze(0), src_audio["sample_rate"], filename_prefix="src_audio_")
        
        audio_duration = librosa.get_duration(filename=src_audio_path)
        repaint_start = -left_extend_length
        repaint_end = audio_duration + right_extend_length

        parameters = ast.literal_eval(parameters)
        parameters["audio_duration"] = audio_duration

        ap = AP(model_path)
        audio_output = ap(
            prompt=prompt, 
            lyrics=lyrics, 
            task="extend", 
            retake_seeds=retake_seeds, 
            src_audio_path=src_audio_path, 
            repaint_start=repaint_start, 
            repaint_end=repaint_end, 
            retake_variance=1.0,
            **parameters)
            
        audio, sr = audio_output[0][0].unsqueeze(0), audio_output[0][1]

        ac.cleanup_file(src_audio_path)
        if unload_model:
            ap.cleanup()
        
        return ({"waveform": audio, "sample_rate": sr},)


NODE_CLASS_MAPPINGS = {
    "ACEStepGen": ACEStepGen,
    "GenerationParameters": GenerationParameters,
    "MultiLinePromptACES": MultiLinePromptACES,
    "MultiLineLyrics": MultiLineLyrics,
    "ACEStepRepainting": ACEStepRepainting,
    "ACEStepEdit": ACEStepEdit,
    "ACEStepExtend": ACEStepExtend,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ACEStepGen": "ACE-Step",
    "GenerationParameters": "ACE-Step Parameters",
    "MultiLinePromptACES": "ACE-Step Prompt",
    "MultiLineLyrics": "ACE-Step Lyrics",
    "ACEStepRepainting": "ACE-Step Repainting",
    "ACEStepEdit": "ACE-Step Edit",
    "ACEStepExtend": "ACE-Step Extend",
}